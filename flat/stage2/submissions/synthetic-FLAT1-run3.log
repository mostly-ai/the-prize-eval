uv not found, installing now...
downloading uv 0.7.19 x86_64-unknown-linux-gnu
no checksums to verify
installing to /home/ubuntu/.local/bin
  uv
  uvx
everything's installed!
uv has been installed successfully.
--- Creating Python virtual environment using uv ---
Using CPython 3.12.3 interpreter at: /usr/bin/python3
Creating virtual environment at: .venv
Activate with: source .venv/bin/activate
--- Installing dependencies from requirements.txt ---
Resolved 248 packages in 203ms
Installed 248 packages in 431ms
 + accelerate==1.8.1
 + adlfs==2024.12.0
 + aiobotocore==2.23.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.12.13
 + aioitertools==0.12.0
 + aiosignal==1.3.2
 + airportsdata==20250622
 + annotated-types==0.7.0
 + anyio==4.9.0
 + astor==0.8.1
 + asttokens==3.0.0
 + attrs==25.3.0
 + azure-common==1.1.28
 + azure-core==1.34.0
 + azure-datalake-store==0.0.53
 + azure-identity==1.23.0
 + azure-storage-blob==12.25.1
 + azure-storage-file-datalake==12.20.0
 + bcrypt==4.3.0
 + bitsandbytes==0.46.0
 + blake3==1.0.5
 + boto3==1.38.27
 + botocore==1.38.27
 + cachetools==5.5.2
 + certifi==2025.6.15
 + cffi==1.17.1
 + charset-normalizer==3.4.2
 + click==8.2.1
 + cloudpathlib==0.21.1
 + cloudpickle==3.1.1
 + comm==0.2.2
 + compressed-tensors==0.9.3
 + contourpy==1.3.2
 + cryptography==45.0.4
 + cupy-cuda12x==13.4.1
 + cycler==0.12.1
 + datasets==3.6.0
 + decorator==5.2.1
 + deprecated==1.2.18
 + depyf==0.18.0
 + dill==0.3.8
 + diskcache==5.6.3
 + distro==1.9.0
 + dnspython==2.7.0
 + duckdb==1.3.1
 + einops==0.8.1
 + email-validator==2.2.0
 + environs==14.2.0
 + et-xmlfile==2.0.0
 + executing==2.2.0
 + fastapi==0.115.14
 + fastapi-cli==0.0.7
 + fastrlock==0.8.3
 + filelock==3.18.0
 + fonttools==4.58.4
 + frozenlist==1.7.0
 + fsspec==2025.3.0
 + gcsfs==2025.3.0
 + gguf==0.17.1
 + google-api-core==2.25.1
 + google-auth==2.40.3
 + google-auth-oauthlib==1.2.2
 + google-cloud-core==2.4.3
 + google-cloud-storage==3.1.1
 + google-crc32c==1.7.1
 + google-resumable-media==2.7.2
 + googleapis-common-protos==1.70.0
 + greenlet==3.2.3
 + grpcio==1.73.1
 + gunicorn==23.0.0
 + h11==0.16.0
 + hf-xet==1.1.5
 + httpcore==1.0.9
 + httptools==0.6.4
 + httpx==0.28.1
 + huggingface-hub==0.33.1
 + idna==3.10
 + importlib-metadata==8.0.0
 + interegular==0.3.3
 + ipython==9.3.0
 + ipython-pygments-lexers==1.1.1
 + ipywidgets==8.1.7
 + isodate==0.7.2
 + jedi==0.19.2
 + jinja2==3.1.6
 + jiter==0.10.0
 + jmespath==1.0.1
 + joblib==1.5.1
 + json-repair==0.46.2
 + jsonschema==4.24.0
 + jsonschema-specifications==2025.4.1
 + jupyterlab-widgets==3.0.15
 + kiwisolver==1.4.8
 + lark==1.2.2
 + llguidance==0.7.30
 + llvmlite==0.44.0
 + lm-format-enforcer==0.10.11
 + markdown-it-py==3.0.0
 + markupsafe==3.0.2
 + marshmallow==4.0.0
 + matplotlib==3.10.3
 + matplotlib-inline==0.1.7
 + mdurl==0.1.2
 + mistral-common==1.6.2
 + model2vec==0.6.0
 + mostlyai==4.7.8
 + mostlyai-engine==1.4.5
 + mostlyai-qa==1.9.7
 + mpmath==1.3.0
 + msal==1.32.3
 + msal-extensions==1.3.1
 + msgpack==1.1.1
 + msgspec==0.19.0
 + multidict==6.6.2
 + multiprocess==0.70.16
 + narwhals==1.44.0
 + nest-asyncio==1.6.0
 + networkx==3.5
 + ninja==1.11.1.4
 + numba==0.61.2
 + numpy==2.2.6
 + nvidia-cublas-cu12==12.4.5.8
 + nvidia-cuda-cupti-cu12==12.4.127
 + nvidia-cuda-nvrtc-cu12==12.4.127
 + nvidia-cuda-runtime-cu12==12.4.127
 + nvidia-cudnn-cu12==9.1.0.70
 + nvidia-cufft-cu12==11.2.1.3
 + nvidia-curand-cu12==10.3.5.147
 + nvidia-cusolver-cu12==11.6.1.9
 + nvidia-cusparse-cu12==12.3.1.170
 + nvidia-cusparselt-cu12==0.6.2
 + nvidia-nccl-cu12==2.21.5
 + nvidia-nvjitlink-cu12==12.4.127
 + nvidia-nvtx-cu12==12.4.127
 + oauthlib==3.3.1
 + opacus==1.5.4
 + openai==1.93.0
 + opencv-python-headless==4.11.0.86
 + openpyxl==3.1.5
 + opentelemetry-api==1.26.0
 + opentelemetry-exporter-otlp==1.26.0
 + opentelemetry-exporter-otlp-proto-common==1.26.0
 + opentelemetry-exporter-otlp-proto-grpc==1.26.0
 + opentelemetry-exporter-otlp-proto-http==1.26.0
 + opentelemetry-proto==1.26.0
 + opentelemetry-sdk==1.26.0
 + opentelemetry-semantic-conventions==0.47b0
 + opentelemetry-semantic-conventions-ai==0.4.9
 + opt-einsum==3.4.0
 + outlines==0.1.11
 + outlines-core==0.1.26
 + packaging==25.0
 + pandas==2.2.3
 + paramiko==3.5.1
 + parso==0.8.4
 + partial-json-parser==0.2.1.1.post6
 + peft==0.15.2
 + pexpect==4.9.0
 + phik==0.12.4
 + pillow==11.2.1
 + plotly==6.2.0
 + prometheus-client==0.22.1
 + prometheus-fastapi-instrumentator==7.1.0
 + prompt-toolkit==3.0.51
 + propcache==0.3.2
 + proto-plus==1.26.1
 + protobuf==4.25.8
 + psutil==5.9.8
 + ptyprocess==0.7.0
 + pure-eval==0.2.3
 + py-cpuinfo==9.0.0
 + pyarrow==20.0.0
 + pyasn1==0.6.1
 + pyasn1-modules==0.4.2
 + pycountry==24.6.1
 + pycparser==2.22
 + pycryptodomex==3.23.0
 + pydantic==2.11.7
 + pydantic-core==2.33.2
 + pygments==2.19.2
 + pyjwt==2.10.1
 + pynacl==1.5.0
 + pyparsing==3.2.3
 + python-dateutil==2.9.0.post0
 + python-dotenv==1.1.1
 + python-json-logger==3.3.0
 + python-multipart==0.0.20
 + pytz==2025.2
 + pyyaml==6.0.2
 + pyzmq==27.0.0
 + ray==2.47.1
 + referencing==0.36.2
 + regex==2024.11.6
 + requests==2.32.4
 + requests-oauthlib==2.0.0
 + rich==14.0.0
 + rich-toolkit==0.14.7
 + rpds-py==0.25.1
 + rsa==4.9.1
 + s3fs==2025.3.0
 + s3transfer==0.13.0
 + safetensors==0.5.3
 + schema==0.7.7
 + scikit-learn==1.7.0
 + scipy==1.15.2
 + semantic-version==2.10.0
 + sentencepiece==0.2.0
 + setuptools==80.9.0
 + shellingham==1.5.4
 + six==1.17.0
 + smart-open==7.1.0
 + sniffio==1.3.1
 + sqlalchemy==2.0.41
 + sqlparse==0.5.3
 + sshtunnel==0.4.0
 + stack-data==0.6.3
 + starlette==0.46.2
 + sympy==1.13.1
 + threadpoolctl==3.6.0
 + tiktoken==0.9.0
 + tokenizers==0.21.2
 + torch==2.6.0
 + torchaudio==2.6.0
 + torchvision==0.21.0
 + tqdm==4.67.1
 + traitlets==5.14.3
 + transformers==4.53.0
 + triton==3.2.0
 + typer==0.16.0
 + typing-extensions==4.14.0
 + typing-inspection==0.4.1
 + tzdata==2025.2
 + urllib3==2.5.0
 + uvicorn==0.34.3
 + uvloop==0.21.0
 + vllm==0.8.5.post1
 + watchfiles==1.1.0
 + wcwidth==0.2.13
 + websockets==15.0.1
 + widgetsnbextension==4.0.14
 + wrapt==1.17.2
 + xformers==0.0.29.post2
 + xgrammar==0.1.18
 + xlsxwriter==3.2.5
 + xxhash==3.5.0
 + yarl==1.20.1
 + zipp==3.23.0
--- Activating the virtual environment ---
--- Running the main prediction script ---
Training with 100000 Train Samples
--- STEP 1: Generating Synthetic Data Pool ---
Initializing Synthetic Data SDK 4.7.8 in LOCAL mode ğŸ 
Connected to /home/ubuntu/mostlyai with 31 GB RAM, 8 CPUs, 1x NVIDIA A10G 
available
Add NA features for: ['rabbit', 'cow', 'loon', 'cloud']
Starting Training Iteration 1/2
Created generator 2fd8e0c5-7033-4e1d-8617-62675f4f406b
Started generator training
<frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.MessageMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
<frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.ScalarMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
Overall job progress                             â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:35:10
Step Baseline:tabular PULL_TRAINING_DATA         â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:06
Step Baseline:tabular ANALYZE_TRAINING_DATA      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:01:00
Step Baseline:tabular ENCODE_TRAINING_DATA       â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:12
Step Baseline:tabular TRAIN_MODEL ğŸ’             â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:31:22
Step Baseline:tabular GENERATE_MODEL_REPORT_DATA â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:10
Step Baseline:tabular CREATE_MODEL_REPORT        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:02:14
                                                                                
                                                                                
                                                                                
                      Training log for `Baseline:tabular`                       
                                                                                
        Epochs              Samples             Elapsed Time          Val Loss  
 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 
         12.00            1,908,579                    1327s          112.2443  
         13.00            2,067,564                    1437s          112.0345  
         14.00            2,226,549                    1547s          111.8521  
         15.00            2,385,534                    1658s          111.6670  
         16.00            2,544,519                    1768s          111.5397  
         17.00            2,703,504                    1878s          111.5291  
                                                                                ğŸ‰ Your generator is ready! Use it to create synthetic data. Publish it so 
others can do the same.
Generating data with generator from iteration 1
Created synthetic dataset 5fcf9295-03b5-40ab-bdc0-f7b5438f9432 with generator 
2fd8e0c5-7033-4e1d-8617-62675f4f406b
Started synthetic dataset generation
<frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.MessageMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
<frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.ScalarMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
Overall job progress                             â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:04:54
Step Baseline:tabular GENERATE_DATA_TABULAR      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:02:22
Step Baseline:tabular CREATE_DATA_REPORT_TABULAR â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:01:30
Step common FINALIZE_GENERATION                  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:58
Step common DELIVER_DATA                         â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:00ğŸ‰ Your synthetic dataset is ready! Use it to consume the generated data. 
Publish it so others can do the same.
Accuracy for iteration 1: {'univariate_accuracy': np.float64(0.978858875), 'bivariate_accuracy': np.float64(0.9637508544303797), 'trivariate_accuracy': np.float64(0.9464780180000002), 'overall_accuracy': np.float64(0.9630292491434599)}
Starting Training Iteration 2/2
Created generator 73cb0c8b-9b99-4bc0-aec0-ae9fa5216146
Started generator training
<frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.MessageMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
<frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.ScalarMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
Overall job progress                             â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:35:06
Step Baseline:tabular PULL_TRAINING_DATA         â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:06
Step Baseline:tabular ANALYZE_TRAINING_DATA      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:01:00
Step Baseline:tabular ENCODE_TRAINING_DATA       â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:12
Step Baseline:tabular TRAIN_MODEL ğŸ’             â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:31:22
Step Baseline:tabular GENERATE_MODEL_REPORT_DATA â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:10
Step Baseline:tabular CREATE_MODEL_REPORT        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:02:12
                                                                                
                                                                                
                                                                                
                      Training log for `Baseline:tabular`                       
                                                                                
        Epochs              Samples             Elapsed Time          Val Loss  
 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 
         12.00            1,908,579                    1326s          112.5944  
         13.00            2,067,564                    1436s          112.4221  
         14.00            2,226,549                    1547s          112.2739  
         15.00            2,385,534                    1658s          112.0487  
         16.00            2,544,519                    1769s          111.9794  
         17.00            2,703,504                    1879s          111.8116  
                                                                                ğŸ‰ Your generator is ready! Use it to create synthetic data. Publish it so 
others can do the same.
Generating data with generator from iteration 2
Created synthetic dataset 4bd5a549-c2f6-45e5-b8bd-6dbf3910f5f3 with generator 
73cb0c8b-9b99-4bc0-aec0-ae9fa5216146
Started synthetic dataset generation
<frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.MessageMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
<frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.ScalarMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
Overall job progress                             â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:04:52
Step Baseline:tabular GENERATE_DATA_TABULAR      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:02:22
Step Baseline:tabular CREATE_DATA_REPORT_TABULAR â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:01:30
Step common FINALIZE_GENERATION                  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:56
Step common DELIVER_DATA                         â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:00ğŸ‰ Your synthetic dataset is ready! Use it to consume the generated data. 
Publish it so others can do the same.
Accuracy for iteration 2: {'univariate_accuracy': np.float64(0.979032875), 'bivariate_accuracy': np.float64(0.9642477943037976), 'trivariate_accuracy': np.float64(0.94708176), 'overall_accuracy': np.float64(0.9634541431012659)}
Combining data from all iterations into the final pool.
Final combined data pool accuracy: {'univariate_accuracy': np.float64(0.981834625), 'bivariate_accuracy': np.float64(0.9682764620253165), 'trivariate_accuracy': np.float64(0.952185022), 'overall_accuracy': np.float64(0.9674320363417722)}
Synthetic data pool saved to pool_data/flat_intermediate_pre_trained_pool_20250708_1101.csv
--- STEP 2: Selecting Best Subset via Post-processing ---
--- Step 0: Binning data for IPF ---
--- Step 1: Generating initial subset with IPF (top 5000 pairs) ---
Function 'ipf_pairs_full' executed in 17.19 minutes.
--- Step 2: Preparing IPF result for refinement ---
IPF returned 125000 indices with 120453 unique ones.
--- Accuracy of IPF-selected subset (before refinement) ---
Overall accuracy: 0.9911181018734178
--- Step 3: Trimming the subset ---
Starting with the provided initial set...
Initial solution error (normalized): 0.103627
Iter  100/100000: Swap Size:  80, Num Elements 107403, Norm. L1 Err: 0.038473, Accuracy vs Original: 0.990651
Iter  200/100000: Swap Size:   5, Num Elements 104053, Norm. L1 Err: 0.022704, Accuracy vs Original: 0.991003
Iter  300/100000: Swap Size:   5, Num Elements 103553, Norm. L1 Err: 0.020530, Accuracy vs Original: 0.991019
Iter  400/100000: Swap Size:   5, Num Elements 103053, Norm. L1 Err: 0.018436, Accuracy vs Original: 0.991116
Iter  500/100000: Swap Size:   5, Num Elements 102553, Norm. L1 Err: 0.016436, Accuracy vs Original: 0.991309
Iter  600/100000: Swap Size:   5, Num Elements 102053, Norm. L1 Err: 0.014536, Accuracy vs Original: 0.991462
Iter  700/100000: Swap Size:   5, Num Elements 101553, Norm. L1 Err: 0.012797, Accuracy vs Original: 0.991594
Iter  800/100000: Swap Size:   5, Num Elements 101053, Norm. L1 Err: 0.011222, Accuracy vs Original: 0.991724
Iter  900/100000: Swap Size:   5, Num Elements 100553, Norm. L1 Err: 0.009929, Accuracy vs Original: 0.991984
Iter 1000/100000: Swap Size:   5, Num Elements 100053, Norm. L1 Err: 0.009142, Accuracy vs Original: 0.992270
Trimming stopped, remove last 2 elements
Finished refinement in 1688.53 seconds.
Function 'choose_rows_by_refinement' executed in 28.14 minutes.
--- Accuracy of Trimming subset (subset size 100000) ---
Overall accuracy: 0.9923247361561182
Trimming returned 100000 indices with 100000 unique ones.
--- Step 4: Refining the subset ---
Current memory consumption: 16.12 GB
Using train set with len 100000 rows to align to refinement target size 100000.
Starting with the provided initial set...
Initial solution error (normalized): 0.009133
Iter  100/500: Swap Size:  44, Num Elements 100000, Norm. L1 Err: 0.007944, Accuracy vs Original: 0.992695
Iter  200/500: Swap Size:  12, Num Elements 100000, Norm. L1 Err: 0.007555, Accuracy vs Original: 0.992722
Iter  300/500: Swap Size:  10, Num Elements 100000, Norm. L1 Err: 0.007359, Accuracy vs Original: 0.992732
Iter  400/500: Swap Size:   5, Num Elements 100000, Norm. L1 Err: 0.007268, Accuracy vs Original: 0.992710
Iter  500/500: Swap Size:   1, Num Elements 100000, Norm. L1 Err: 0.007238, Accuracy vs Original: 0.992752
Finished refinement in 746.54 seconds.
Function 'choose_rows_by_refinement' executed in 12.44 minutes.
Function 'select_rows_with_ipf_and_refinement' executed in 59.30 minutes.
--- STEP 3: Final Evaluation ---
Using Local Validation Metrics
Accuracy of Initial Pool (Before):
  - univariate_accuracy: 0.9822444999999999
  - bivariate_accuracy: 0.9689352151898735
  - trivariate_accuracy: 0.9526092270000001
  - overall_accuracy: 0.9679296473966245
Accuracy of Refined Subset (After):
  - univariate_accuracy: 0.999099875
  - bivariate_accuracy: 0.995423259493671
  - trivariate_accuracy: 0.9833769119999999
  - overall_accuracy: 0.9926333488312236
Quick Checks on Final Subset:
  - Size: 100000
  - All unique indices: True
  - All unique data points: True
--- STEP 4: Storing Final Result ---
Final output saved to results/flat_result_20250708_1101.csv
Pipeline completed successfully in 2.41 hours.
--- Script finished successfully ---
