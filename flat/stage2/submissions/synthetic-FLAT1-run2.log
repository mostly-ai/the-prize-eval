uv not found, installing now...
downloading uv 0.7.19 x86_64-unknown-linux-gnu
no checksums to verify
installing to /home/ubuntu/.local/bin
  uv
  uvx
everything's installed!
uv has been installed successfully.
--- Creating Python virtual environment using uv ---
Using CPython 3.12.3 interpreter at: /usr/bin/python3
Creating virtual environment at: .venv
Activate with: source .venv/bin/activate
--- Installing dependencies from requirements.txt ---
Resolved 248 packages in 207ms
Installed 248 packages in 431ms
 + accelerate==1.8.1
 + adlfs==2024.12.0
 + aiobotocore==2.23.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.12.13
 + aioitertools==0.12.0
 + aiosignal==1.3.2
 + airportsdata==20250622
 + annotated-types==0.7.0
 + anyio==4.9.0
 + astor==0.8.1
 + asttokens==3.0.0
 + attrs==25.3.0
 + azure-common==1.1.28
 + azure-core==1.34.0
 + azure-datalake-store==0.0.53
 + azure-identity==1.23.0
 + azure-storage-blob==12.25.1
 + azure-storage-file-datalake==12.20.0
 + bcrypt==4.3.0
 + bitsandbytes==0.46.0
 + blake3==1.0.5
 + boto3==1.38.27
 + botocore==1.38.27
 + cachetools==5.5.2
 + certifi==2025.6.15
 + cffi==1.17.1
 + charset-normalizer==3.4.2
 + click==8.2.1
 + cloudpathlib==0.21.1
 + cloudpickle==3.1.1
 + comm==0.2.2
 + compressed-tensors==0.9.3
 + contourpy==1.3.2
 + cryptography==45.0.4
 + cupy-cuda12x==13.4.1
 + cycler==0.12.1
 + datasets==3.6.0
 + decorator==5.2.1
 + deprecated==1.2.18
 + depyf==0.18.0
 + dill==0.3.8
 + diskcache==5.6.3
 + distro==1.9.0
 + dnspython==2.7.0
 + duckdb==1.3.1
 + einops==0.8.1
 + email-validator==2.2.0
 + environs==14.2.0
 + et-xmlfile==2.0.0
 + executing==2.2.0
 + fastapi==0.115.14
 + fastapi-cli==0.0.7
 + fastrlock==0.8.3
 + filelock==3.18.0
 + fonttools==4.58.4
 + frozenlist==1.7.0
 + fsspec==2025.3.0
 + gcsfs==2025.3.0
 + gguf==0.17.1
 + google-api-core==2.25.1
 + google-auth==2.40.3
 + google-auth-oauthlib==1.2.2
 + google-cloud-core==2.4.3
 + google-cloud-storage==3.1.1
 + google-crc32c==1.7.1
 + google-resumable-media==2.7.2
 + googleapis-common-protos==1.70.0
 + greenlet==3.2.3
 + grpcio==1.73.1
 + gunicorn==23.0.0
 + h11==0.16.0
 + hf-xet==1.1.5
 + httpcore==1.0.9
 + httptools==0.6.4
 + httpx==0.28.1
 + huggingface-hub==0.33.1
 + idna==3.10
 + importlib-metadata==8.0.0
 + interegular==0.3.3
 + ipython==9.3.0
 + ipython-pygments-lexers==1.1.1
 + ipywidgets==8.1.7
 + isodate==0.7.2
 + jedi==0.19.2
 + jinja2==3.1.6
 + jiter==0.10.0
 + jmespath==1.0.1
 + joblib==1.5.1
 + json-repair==0.46.2
 + jsonschema==4.24.0
 + jsonschema-specifications==2025.4.1
 + jupyterlab-widgets==3.0.15
 + kiwisolver==1.4.8
 + lark==1.2.2
 + llguidance==0.7.30
 + llvmlite==0.44.0
 + lm-format-enforcer==0.10.11
 + markdown-it-py==3.0.0
 + markupsafe==3.0.2
 + marshmallow==4.0.0
 + matplotlib==3.10.3
 + matplotlib-inline==0.1.7
 + mdurl==0.1.2
 + mistral-common==1.6.2
 + model2vec==0.6.0
 + mostlyai==4.7.8
 + mostlyai-engine==1.4.5
 + mostlyai-qa==1.9.7
 + mpmath==1.3.0
 + msal==1.32.3
 + msal-extensions==1.3.1
 + msgpack==1.1.1
 + msgspec==0.19.0
 + multidict==6.6.2
 + multiprocess==0.70.16
 + narwhals==1.44.0
 + nest-asyncio==1.6.0
 + networkx==3.5
 + ninja==1.11.1.4
 + numba==0.61.2
 + numpy==2.2.6
 + nvidia-cublas-cu12==12.4.5.8
 + nvidia-cuda-cupti-cu12==12.4.127
 + nvidia-cuda-nvrtc-cu12==12.4.127
 + nvidia-cuda-runtime-cu12==12.4.127
 + nvidia-cudnn-cu12==9.1.0.70
 + nvidia-cufft-cu12==11.2.1.3
 + nvidia-curand-cu12==10.3.5.147
 + nvidia-cusolver-cu12==11.6.1.9
 + nvidia-cusparse-cu12==12.3.1.170
 + nvidia-cusparselt-cu12==0.6.2
 + nvidia-nccl-cu12==2.21.5
 + nvidia-nvjitlink-cu12==12.4.127
 + nvidia-nvtx-cu12==12.4.127
 + oauthlib==3.3.1
 + opacus==1.5.4
 + openai==1.93.0
 + opencv-python-headless==4.11.0.86
 + openpyxl==3.1.5
 + opentelemetry-api==1.26.0
 + opentelemetry-exporter-otlp==1.26.0
 + opentelemetry-exporter-otlp-proto-common==1.26.0
 + opentelemetry-exporter-otlp-proto-grpc==1.26.0
 + opentelemetry-exporter-otlp-proto-http==1.26.0
 + opentelemetry-proto==1.26.0
 + opentelemetry-sdk==1.26.0
 + opentelemetry-semantic-conventions==0.47b0
 + opentelemetry-semantic-conventions-ai==0.4.9
 + opt-einsum==3.4.0
 + outlines==0.1.11
 + outlines-core==0.1.26
 + packaging==25.0
 + pandas==2.2.3
 + paramiko==3.5.1
 + parso==0.8.4
 + partial-json-parser==0.2.1.1.post6
 + peft==0.15.2
 + pexpect==4.9.0
 + phik==0.12.4
 + pillow==11.2.1
 + plotly==6.2.0
 + prometheus-client==0.22.1
 + prometheus-fastapi-instrumentator==7.1.0
 + prompt-toolkit==3.0.51
 + propcache==0.3.2
 + proto-plus==1.26.1
 + protobuf==4.25.8
 + psutil==5.9.8
 + ptyprocess==0.7.0
 + pure-eval==0.2.3
 + py-cpuinfo==9.0.0
 + pyarrow==20.0.0
 + pyasn1==0.6.1
 + pyasn1-modules==0.4.2
 + pycountry==24.6.1
 + pycparser==2.22
 + pycryptodomex==3.23.0
 + pydantic==2.11.7
 + pydantic-core==2.33.2
 + pygments==2.19.2
 + pyjwt==2.10.1
 + pynacl==1.5.0
 + pyparsing==3.2.3
 + python-dateutil==2.9.0.post0
 + python-dotenv==1.1.1
 + python-json-logger==3.3.0
 + python-multipart==0.0.20
 + pytz==2025.2
 + pyyaml==6.0.2
 + pyzmq==27.0.0
 + ray==2.47.1
 + referencing==0.36.2
 + regex==2024.11.6
 + requests==2.32.4
 + requests-oauthlib==2.0.0
 + rich==14.0.0
 + rich-toolkit==0.14.7
 + rpds-py==0.25.1
 + rsa==4.9.1
 + s3fs==2025.3.0
 + s3transfer==0.13.0
 + safetensors==0.5.3
 + schema==0.7.7
 + scikit-learn==1.7.0
 + scipy==1.15.2
 + semantic-version==2.10.0
 + sentencepiece==0.2.0
 + setuptools==80.9.0
 + shellingham==1.5.4
 + six==1.17.0
 + smart-open==7.1.0
 + sniffio==1.3.1
 + sqlalchemy==2.0.41
 + sqlparse==0.5.3
 + sshtunnel==0.4.0
 + stack-data==0.6.3
 + starlette==0.46.2
 + sympy==1.13.1
 + threadpoolctl==3.6.0
 + tiktoken==0.9.0
 + tokenizers==0.21.2
 + torch==2.6.0
 + torchaudio==2.6.0
 + torchvision==0.21.0
 + tqdm==4.67.1
 + traitlets==5.14.3
 + transformers==4.53.0
 + triton==3.2.0
 + typer==0.16.0
 + typing-extensions==4.14.0
 + typing-inspection==0.4.1
 + tzdata==2025.2
 + urllib3==2.5.0
 + uvicorn==0.34.3
 + uvloop==0.21.0
 + vllm==0.8.5.post1
 + watchfiles==1.1.0
 + wcwidth==0.2.13
 + websockets==15.0.1
 + widgetsnbextension==4.0.14
 + wrapt==1.17.2
 + xformers==0.0.29.post2
 + xgrammar==0.1.18
 + xlsxwriter==3.2.5
 + xxhash==3.5.0
 + yarl==1.20.1
 + zipp==3.23.0
--- Activating the virtual environment ---
--- Running the main prediction script ---
Training with 100000 Train Samples
--- STEP 1: Generating Synthetic Data Pool ---
Initializing Synthetic Data SDK 4.7.8 in LOCAL mode ğŸ 
Connected to /home/ubuntu/mostlyai with 31 GB RAM, 8 CPUs, 1x NVIDIA A10G 
available
Add NA features for: ['rabbit', 'cow', 'loon', 'cloud']
Starting Training Iteration 1/2
Created generator ac4f2553-3d82-4b0b-9743-82e8ef82ef91
Started generator training
<frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.MessageMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
<frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.ScalarMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
Overall job progress                             â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:35:12
Step Baseline:tabular PULL_TRAINING_DATA         â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:06
Step Baseline:tabular ANALYZE_TRAINING_DATA      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:01:00
Step Baseline:tabular ENCODE_TRAINING_DATA       â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:14
Step Baseline:tabular TRAIN_MODEL ğŸ’             â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:31:24
Step Baseline:tabular GENERATE_MODEL_REPORT_DATA â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:10
Step Baseline:tabular CREATE_MODEL_REPORT        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:02:12
                                                                                
                                                                                
                                                                                
                      Training log for `Baseline:tabular`                       
                                                                                
        Epochs              Samples             Elapsed Time          Val Loss  
 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 
         12.00            1,908,579                    1328s          112.1678  
         13.00            2,067,564                    1439s          111.9710  
         14.00            2,226,549                    1549s          111.7650  
         15.00            2,385,534                    1660s          111.7443  
         16.00            2,544,519                    1771s          111.5330  
         17.00            2,703,504                    1881s          111.4018  
                                                                                ğŸ‰ Your generator is ready! Use it to create synthetic data. Publish it so 
others can do the same.
Generating data with generator from iteration 1
Created synthetic dataset 927ce50f-8a27-444d-a0ec-a99e16fb67a8 with generator 
ac4f2553-3d82-4b0b-9743-82e8ef82ef91
Started synthetic dataset generation
<frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.MessageMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
<frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.ScalarMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
Overall job progress                             â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:04:50
Step Baseline:tabular GENERATE_DATA_TABULAR      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:02:20
Step Baseline:tabular CREATE_DATA_REPORT_TABULAR â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:01:30
Step common FINALIZE_GENERATION                  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:56
Step common DELIVER_DATA                         â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:00ğŸ‰ Your synthetic dataset is ready! Use it to consume the generated data. 
Publish it so others can do the same.
Accuracy for iteration 1: {'univariate_accuracy': np.float64(0.976934), 'bivariate_accuracy': np.float64(0.9608880759493671), 'trivariate_accuracy': np.float64(0.9433718699999999), 'overall_accuracy': np.float64(0.9603979819831223)}
Starting Training Iteration 2/2
Created generator 2b110cd2-57d5-44b3-9da5-24d931ada80b
Started generator training
<frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.MessageMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
<frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.ScalarMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
Overall job progress                             â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:35:02
Step Baseline:tabular PULL_TRAINING_DATA         â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:06
Step Baseline:tabular ANALYZE_TRAINING_DATA      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:01:00
Step Baseline:tabular ENCODE_TRAINING_DATA       â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:12
Step Baseline:tabular TRAIN_MODEL ğŸ’             â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:31:14
Step Baseline:tabular GENERATE_MODEL_REPORT_DATA â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:10
Step Baseline:tabular CREATE_MODEL_REPORT        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:02:14
                                                                                
                                                                                
                                                                                
                      Training log for `Baseline:tabular`                       
                                                                                
        Epochs              Samples             Elapsed Time          Val Loss  
 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 
         12.00            1,908,579                    1322s          112.4703  
         13.00            2,067,564                    1432s          112.2477  
         14.00            2,226,549                    1543s          111.9584  
         15.00            2,385,534                    1653s          111.8265  
         16.00            2,544,519                    1763s          111.6862  
         17.00            2,703,504                    1873s          111.6025  
                                                                                ğŸ‰ Your generator is ready! Use it to create synthetic data. Publish it so 
others can do the same.
Generating data with generator from iteration 2
Created synthetic dataset 76761e3a-400f-4c13-b142-c0befa880ec5 with generator 
2b110cd2-57d5-44b3-9da5-24d931ada80b
Started synthetic dataset generation
<frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.MessageMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
<frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.ScalarMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
Overall job progress                             â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:04:50
Step Baseline:tabular GENERATE_DATA_TABULAR      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:02:20
Step Baseline:tabular CREATE_DATA_REPORT_TABULAR â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:01:30
Step common FINALIZE_GENERATION                  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:56
Step common DELIVER_DATA                         â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:00ğŸ‰ Your synthetic dataset is ready! Use it to consume the generated data. 
Publish it so others can do the same.
Accuracy for iteration 2: {'univariate_accuracy': np.float64(0.9808356250000001), 'bivariate_accuracy': np.float64(0.9665492974683544), 'trivariate_accuracy': np.float64(0.949473257), 'overall_accuracy': np.float64(0.9656193931561182)}
Combining data from all iterations into the final pool.
Final combined data pool accuracy: {'univariate_accuracy': np.float64(0.981804625), 'bivariate_accuracy': np.float64(0.9680711645569621), 'trivariate_accuracy': np.float64(0.9518591399999999), 'overall_accuracy': np.float64(0.9672449765189873)}
Synthetic data pool saved to pool_data/flat_intermediate_pre_trained_pool_20250708_0836.csv
--- STEP 2: Selecting Best Subset via Post-processing ---
--- Step 0: Binning data for IPF ---
--- Step 1: Generating initial subset with IPF (top 5000 pairs) ---
Function 'ipf_pairs_full' executed in 17.25 minutes.
--- Step 2: Preparing IPF result for refinement ---
IPF returned 125000 indices with 120532 unique ones.
--- Accuracy of IPF-selected subset (before refinement) ---
Overall accuracy: 0.990921090029536
--- Step 3: Trimming the subset ---
Starting with the provided initial set...
Initial solution error (normalized): 0.103983
Iter  100/100000: Swap Size:  80, Num Elements 107482, Norm. L1 Err: 0.038783, Accuracy vs Original: 0.990830
Iter  200/100000: Swap Size:   5, Num Elements 104132, Norm. L1 Err: 0.022887, Accuracy vs Original: 0.991169
Iter  300/100000: Swap Size:   5, Num Elements 103632, Norm. L1 Err: 0.020672, Accuracy vs Original: 0.991312
Iter  400/100000: Swap Size:   5, Num Elements 103132, Norm. L1 Err: 0.018536, Accuracy vs Original: 0.991374
Iter  500/100000: Swap Size:   5, Num Elements 102632, Norm. L1 Err: 0.016485, Accuracy vs Original: 0.991404
Iter  600/100000: Swap Size:   5, Num Elements 102132, Norm. L1 Err: 0.014548, Accuracy vs Original: 0.991620
Iter  700/100000: Swap Size:   5, Num Elements 101632, Norm. L1 Err: 0.012751, Accuracy vs Original: 0.991686
Iter  800/100000: Swap Size:   5, Num Elements 101132, Norm. L1 Err: 0.011154, Accuracy vs Original: 0.991912
Iter  900/100000: Swap Size:   5, Num Elements 100632, Norm. L1 Err: 0.009805, Accuracy vs Original: 0.992173
Iter 1000/100000: Swap Size:   5, Num Elements 100132, Norm. L1 Err: 0.008886, Accuracy vs Original: 0.992489
Trimming stopped, remove last 2 elements
Finished refinement in 1691.43 seconds.
Function 'choose_rows_by_refinement' executed in 28.19 minutes.
--- Accuracy of Trimming subset (subset size 100000) ---
Overall accuracy: 0.9924684745527426
Trimming returned 100000 indices with 100000 unique ones.
--- Step 4: Refining the subset ---
Current memory consumption: 15.99 GB
Using train set with len 100000 rows to align to refinement target size 100000.
Starting with the provided initial set...
Initial solution error (normalized): 0.008817
Iter  100/500: Swap Size:  44, Num Elements 100000, Norm. L1 Err: 0.007670, Accuracy vs Original: 0.992787
Iter  200/500: Swap Size:  18, Num Elements 100000, Norm. L1 Err: 0.007310, Accuracy vs Original: 0.992778
Iter  300/500: Swap Size:  22, Num Elements 100000, Norm. L1 Err: 0.007097, Accuracy vs Original: 0.992830
Iter  400/500: Swap Size:  10, Num Elements 100000, Norm. L1 Err: 0.006991, Accuracy vs Original: 0.992762
Iter  500/500: Swap Size:   1, Num Elements 100000, Norm. L1 Err: 0.006980, Accuracy vs Original: 0.992734
Finished refinement in 745.54 seconds.
Function 'choose_rows_by_refinement' executed in 12.43 minutes.
Function 'select_rows_with_ipf_and_refinement' executed in 59.40 minutes.
--- STEP 3: Final Evaluation ---
Using Local Validation Metrics
Accuracy of Initial Pool (Before):
  - univariate_accuracy: 0.9814018750000001
  - bivariate_accuracy: 0.9676559936708862
  - trivariate_accuracy: 0.9512037959999999
  - overall_accuracy: 0.9667538882236286
Accuracy of Refined Subset (After):
  - univariate_accuracy: 0.999114625
  - bivariate_accuracy: 0.9954786265822784
  - trivariate_accuracy: 0.9837227939999998
  - overall_accuracy: 0.9927720151940927
Quick Checks on Final Subset:
  - Size: 100000
  - All unique indices: True
  - All unique data points: True
--- STEP 4: Storing Final Result ---
Final output saved to results/flat_result_20250708_0836.csv
Pipeline completed successfully in 2.41 hours.
--- Script finished successfully ---
